---
aliases:
cssclasses:
  - mercoledi
tags:
  - configmap
  - appsettings
  - volumi
---
mercoledi

9:30-11:30 corso HSE (salute sicurezza e ambiente)

pomeriggio






## differenze configmap e appsettings

```yaml

apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config

data:
  appsettings.json: |

    {
      "Logging": {
        "LogLevel": {
          "Default": "Information"

        }
      }
    }
```

...

in ogni deploy c'e una config map che si chiama.. app-config 
dovrebbe contenere le variabili che tutti i log che servono all'intera app, comunr a tutti... adesso ce n'è uno in ogni deploy di ogni microservizio..

vecchio metodo aveva problema. repo apposta che conteneva tutti i deploy dei servizi e una cartella Config_and_Secrets con tutte le variabili e configmap-general. aveva dei contro.


in questo modo la configmap è troppo "segmentata"...

invece nell'appsettings cosa c'e?

le variabili del progetto vanno a leggere i loro valori in un file. che è dentro restsrv.. in attachments è dentro a deploy... ma è solo lì, solo in quell'applicativo. gli altri repo vanno a leggere i valori dall'unico appsettings esistente che è quello di attachments... la soluzione sarà generare un appsettings (con nome diverso) per ogni applicativo... puo stare dentro a kubernetes come `appsettings-configmap.yaml` e dentro nei metadata dargli un `name` diverso.. 

## come
1. genero appsettings da riga di comando, lo fa lui strano -> yaml

2. lo carico sulla configmap.. (come se ci fosse un entry per appsettinggs che contiene le informazioni di data: appsetting.json)


3. alla fine lo ritrasformo in un un file yaml per far si che non serva ribuildare tutto ad ogni modifica (estraggo l'appsettings e lo riuso nella configmap)


----
in italiano
#### Situazione attuale: ConfigMap “troppo segmentata”

- Ogni microservizio ha **la propria ConfigMap**, contenente le variabili necessarie al logging, ai percorsi, alle chiavi, ecc.
    
- Problema: **duplicazione e gestione complicata**, perché alcune variabili sono comuni a tutti i microservizi, ma ora sono replicate in ogni ConfigMap.
    
- Questo porta a incoerenze, aggiornamenti manuali multipli e difficoltà di manutenzione.
    

---

#### Vecchio metodo

- C’era un **repository centralizzato** con tutti i deploy dei servizi.
    
- All’interno, una cartella `Config_and_Secrets` conteneva **tutte le variabili comuni** e una ConfigMap generale (`configmap-general`).
    
- Vantaggio: centralizzazione → variabili comuni erano in un solo posto.
    
- Svantaggio: repository enorme e complesso da gestire, eventuali modifiche richiedevano rebuild o deploy di tutti i servizi, anche quelli non interessati.
    

---

####  Cosa c’è nell’`appsettings.json`

- In .NET (o in generale), l’`appsettings.json` contiene **tutte le variabili di configurazione di un singolo progetto**:
    - stringhe di connessione        
    - parametri di logging
    - percorsi e endpoint di altri servizi      
- Ogni progetto legge i propri valori da un `appsettings.json` presente nella sua cartella (ad esempio `restsrv` o `attachments/deploy`).
    

---

####  Problema attuale con gli altri repository

- Alcuni progetti **leggono valori da un appsettings centrale**, ad esempio quello di `attachments`    
- Questo significa che tutti i microservizi condividono lo stesso `appsettings.json`, anche se alcune variabili sono specifiche di un’applicazione.    
- Rischio: un cambiamento in un’app può involontariamente influire sugli altri servizi.
    

---

#### Soluzione proposta

**Obiettivo:** generare un `appsettings.json` separato per ogni microservizio, ma mantenerlo gestibile dentro Kubernetes senza dover ricostruire l’immagine ad ogni modifica.

#### Passaggi:

1. **Generare un appsettings per ogni applicativo**
    - Creare un file con nome specifico, ad esempio `installedparts-appsettings.json` o `partnumbers-appsettings.json`.    
    - La generazione può avvenire da riga di comando, tramite uno script che converte le variabili in YAML pronto per Kubernetes.
        
2. **Caricare in Kubernetes come ConfigMap**
    - Creare un file `appsettings-configmap.yaml` per ogni servizio.        
    - Metadata: `name: installedparts-appsettings` (nome diverso per ogni servizio)
    - Contenuto: `data: appsettings.json: <contenuto del file JSON>`
    - In questo modo, ogni servizio legge dalla sua ConfigMap dedicata, senza conflitti.
        
3. **Uso senza rebuild**
    - Estrarre il contenuto dell’`appsettings.json` e inserirlo nella ConfigMap YAML.
    - Modificando la ConfigMap in Kubernetes, non serve rebuildare l’immagine docker: il servizio legge la ConfigMap al runtime o al deploy.
    - Questo mantiene aggiornabile la configurazione in modo centralizzato, ma **segmentato per servizio**, evitando duplicazioni o conflitti.

---

#### Vantaggi della nuova soluzione

- Ogni microservizio ha il **suo appsettings separato**, quindi variabili specifiche non confliggono.
- La configurazione **può essere aggiornata senza rebuild**.
- Mantiene il vantaggio del vecchio approccio centralizzato per le variabili comuni (possono essere replicate o generate automaticamente in ogni ConfigMap).
- Pulizia e manutenzione più semplice: cambi solo la ConfigMap interessata, senza toccare altre applicazioni.


#### nel deployment
ci sono gia le regole per salvare l'appsettings nel volume (persistent), lo va a copiare li dentro.
volumemount contiene i comandi per la creazioni dei pv (persistent volumi)...
### volumi
li creo al di fuori.. in modo che quando il pod muore il file dentro a volume ha sempre la sua copia a cui il pod può puntare 

monta l'appsetting dentro a /tmp (comando mount) quindi sa dove va a cercare e se lo copia per usarlo...











