
risolvere problemi di deploy di backend e attachments


vedi chat
riassunto di ieri
###  Pipeline e Dockerfile

- la pipeline di PartNumbers** (da InstalledParts**.
- Dockerfile dei due repository era identico, ma la pipeline di `InstalledParts` falliva durante la fase di build multiarch.
- Errore principale:

```
unable to find user ContainerUser: invalid argument
InvalidBaseImagePlatform: Base image ...-nanoserver-1809 was pulled with platform "windows" expected "linux"
```

- Analisi:
    - L’immagine Docker di `InstalledParts` era **Windows (`nanoserver-1809`)**, mentre la pipeline usa **agent Linux (`ubuntu-latest`)** e build multiarch Linux/ARM.
        
    - `PartNumbers` funzionava perché usa immagine Linux.
- Soluzione:
    
    - Cambiare il Dockerfile di InstalledParts a immagini Linux:
        
        - `mcr.microsoft.com/dotnet/aspnet:8.0` (base)
            
        - `mcr.microsoft.com/dotnet/sdk:8.0` (build)
            
    - Modificare `%BUILD_CONFIGURATION%` (Windows) in `$BUILD_CONFIGURATION` (Linux).
        
    - Questo evita errori ContainerUser e InvalidBaseImagePlatform.
        

---


## Backend_IAS e Attachments 
### Gestione ConfigMap / appsettings

- Problema:
    - Ogni microservizio ha la propria ConfigMap, molte variabili comuni duplicate → gestione complicata.
    - Alcuni servizi leggono `appsettings.json` di un altro progetto (ad esempio `attachments`) → rischi di conflitto.
- Vecchio metodo:
    - Repository centralizzato con tutte le variabili in `Config_and_Secrets` e una configmap generale.
    - Vantaggio: centralizzazione.    
    - Svantaggio: repository enorme e rebuild obbligatori.
        
- Soluzione proposta:
    - Creare un `appsettings.json` dedicato per ogni microservizio.
    - Trasformarlo in ConfigMap YAML (appsettings-configmap.yaml) con `name` diverso per ogni servizio.
	
    - Procedura:
        
        1. Generare appsettings da riga di comando.
        2. Caricarlo in Kubernetes come ConfigMap (`data: appsettings.json: <contenuto>`).
        3. Estrarre/riutilizzare il file YAML senza rebuild dell’immagine.
            

---

### Problemi riscontrati con il pod `afm-ms-backend`

- Dopo aver creato la ConfigMap `appsettings-backend-ias` e aggiornato il deployment.yaml, il pod rimaneva `Init:0/1` o `Pending`.
- Log iniziali mostravano errore vecchio nome ConfigMap (`appsettings-config`), ma il deployment era già aggiornato.
    
- Analisi finale:
    - L’errore reale era dovuto al PVC `afm-ms-backend-pvc` già montato su un altro nodo.   
    - Kubernetes non permette mount contemporaneo del volume ReadWriteOnce su più nodi → pod resta `Pending`.
    - La ConfigMap era corretta, il problema era solo il blocco del PVC.
        
- Soluzione:
    - Verificare PVC con `kubectl get pvc -n vvf` e `kubectl describe pvc`.        
	 - Liberare il PVC dai pod vecchi (`kubectl delete pod <vecchio-pod> -n vvf`).        
    - Fare rollout del deployment aggiornato: `kubectl rollout restart deployment afm-ms-backend -n vvf`.
        

- **PVC già montato → pod Pending** → problema infrastrutturale, non di ConfigMap.
- Namespace e rollout vanno sempre verificati per applicare correttamente le ConfigMap.
    


kubectl describe pvc
kubectl get pvc -n vvf




oggi.
PVC e Pod stanno finendo in namespace diversi**, e questo spiega perfettamente perché i pod in `vvf` non riescono a partire correttamente.

---
## Cosa sta succedendo davvero

#### I PVC sono nel namespace `default`

Dai comandi:
```bash
kubectl get pvc -n vvf
```
→ **non compare `afm-ms-backend-pvc`**

Poi:
```bash
kubectl describe pvc afm-ms-backend-pvc
```
risultato:
```
Namespace: default
```
Tutti i PVC che hai descritto sono nel namespace `default`, non in `vvf`.

---
#### Il Deployment è nel namespace `vvf`
Il tuo deployment:
```yaml
metadata:
  name: afm-ms-backend
  namespace: vvf
```

Kubernetes **non può montare un PVC di `default` in un Pod di `vvf`**  
I PVC **sono namespace-scoped**, non globali.

Questo causa:
- Pod in stato `Pending`
- Init container che non parte
- Errori di mount apparentemente “strani”

---
####  Perché vedi PVC “Used By” anche se non parte

Esempio:
```
Used By: afm-ms-backend-5559676d8d-llnhm
```
Questo succede perché:
- Kubernetes prova ad associare il PVC
- Ma **il mount fallisce a runtime**
- Il pod resta bloccato in `Init:0/1` o `Pending`

---

## SOLUZIONE  

Usa solo il namespace `vvf`** per TUTTO
##### Passaggi:
1. Elimina i PVC sbagliati in `default`
(se non contengono dati importanti)
```bash
kubectl delete pvc afm-ms-backend-pvc -n default
```

2. Ricrea i PVC nel namespace `vvf`
Assicurati che il PVC YAML abbia:
```yaml
metadata:
  name: afm-ms-backend-pvc
  namespace: vvf
```

e poi:
```bash
kubectl apply -f afm-ms-backend-pvc.yaml
```

3. Riavvia il deployment
```bash
kubectl rollout restart deployment afm-ms-backend -n vvf
```

 Risultato:
- PVC e Pod nello stesso namespace
- Mount corretto
- InitContainer parte
- App legge l’appsettings
    


> **PVC, ConfigMap, Secret e Pod DEVONO stare nello stesso namespace**

---

### Come dire a kubectl di usare SEMPRE `vvf`
Sì, si può fare ed è fondamentale nel nostro caso.

### Impostare il namespace di default nel context (CONSIGLIATO)
```bash
kubectl config set-context --current --namespace=vvf
```

Da ora in poi:
```bash
kubectl get pods
kubectl get pvc
kubectl apply -f deployment.yaml
```

 tutto verrà eseguito automaticamente in `vvf`.
Verifica:
```bash
kubectl config view --minify | findstr namespace
```


quindi
Dai comandi che hai incollato si vede chiaramente che:
- tutti i PVC sono nel namespace `default`
- i pod che li usano sono nel namespace `default`
- tu invece stai lavorando (correttamente) con i **deployment nel namespace `vvf`**
    
> **PVC e Pod DEVONO stare nello stesso namespace**  

Kubernetes non permette a un pod in `vvf` di montare un PVC che vive in `default`.

prima 
- tutto (pod + pvc) era in `default`
- oppure alcuni servizi sono ancora in `default`
- ora stai **migrando correttamente verso `vvf`**, ma i volumi sono rimasti indietro
    

###  Domanda chiave: “Devo tirare giù i pod prima di cancellare i PVC?”
**SÌ, ASSOLUTAMENTE SÌ**

Motivo tecnico:
- i PVC sono **RWO (ReadWriteOnce)**
- se un pod li sta usando:
    - Kubernetes non te li lascia cancellare
    - oppure li lascia in stato “Terminating”
    - oppure crea caos sugli attach/detach
        

### Ordine corretto (IMPORTANTISSIMO)
1️.  **Cancelli prima i pod / deployment**

```bash
kubectl delete deployment afm-ms-backend -n default
```

oppure (se è pod singolo):

```bash
kubectl delete pod afm-ms-backend-5559676d8d-llnhm -n default
```

2️.  Aspetti che il PVC NON sia più “Used By”
```bash
kubectl describe pvc afm-ms-backend-pvc -n default
```

Deve sparire:
```
Used By: <none>
```

3️.  **Solo ora cancelli il PVC**

```bash
kubectl delete pvc afm-ms-backend-pvc -n default
```

> Mai cancellare un PVC mentre è ancora usato da un pod

---
4. Strategia consigliata (pulita e sicura)

> Tutto in `vvf`, **default non usato**

## Passi consigliati

1. Imposta `vvf` come namespace di default nel tuo terminale

Questo è FONDAMENTALE per evitare errori futuri 

```bash
kubectl config set-context --current --namespace=vvf
```

Verifica:
```bash
kubectl config view --minify | findstr namespace
```

Da ora in poi:

- `kubectl get pods`
    
- `kubectl get pvc`
    
- `kubectl apply -f ...`
    

andranno **automaticamente su `vvf`**

---

####  Ricrea i PVC nel namespace `vvf`

Esempio:

```bash
kubectl apply -f afm-ms-backend-pvc.yaml
```

Controlla:

```bash
kubectl get pvc
```

Deve comparire:

```
afm-ms-backend-pvc   Bound   ...   vvf
```

---

####  Deploy del servizio (in `vvf`)

```bash
kubectl apply -f deployment.yaml
kubectl rollout status deployment afm-ms-backend
```

---

##  Regola d’oro per il futuro (da ricordare)
**Namespace = confine logico completo**
- Pod
- PVC
- ConfigMap
- Secret
- Service

**DEVONO sempre stare nello stesso namespace**

Se no:
- Pending
- Init:0/1
- AttachVolume error   
- MountVolume failed
    





# volumes prima di deploy
Se applichi il Deployment **senza avere il PVC già presente nel namespace `vvf`**, ecco cosa succede passo passo:

---

### 1️⃣ Stato del pod

- Kubernetes tenterà di montare il volume `afm-ms-backend-storage` dal PVC `afm-ms-backend-pvc`.
    
- **Il PVC non esiste → il pod non può partire**.
    
- Lo stato del pod sarà:
    

```
Pending
```

e facendo:

```bash
kubectl describe pod <nome-pod> -n vvf
```

vedrai un messaggio tipo:

```
Warning  FailedMount  Unable to mount volumes for pod "afm-ms-backend-xxxx": persistentvolumeclaim "afm-ms-backend-pvc" not found
```

---

### 2️⃣ Kubernetes **non crea automaticamente il PVC**

- Il Deployment **non genera PVC automaticamente** a meno che tu non stia usando un **StatefulSet con volumeClaimTemplate**.
    
- Nel tuo YAML è un Deployment standard, quindi **devi creare manualmente il PVC** prima di applicare il Deployment.
    

---

### 3️⃣ ConfigMap

- Anche la ConfigMap `appsettings-backend-ias` deve esistere prima.
    
- Se non esiste, il pod rimane in `Init:0/1` o `Pending` perché l’initContainer non trova il file da copiare.
    

---

###  Sintesi

- Senza PVC e ConfigMap, il pod **non si avvierà**.
    
- Lo stato sarà `Pending` o `Init:0/1`.
    
- Kubernetes **non crea PVC o ConfigMap da solo**.
    
- Devi **creare prima il PVC e la ConfigMap** nel namespace corretto (`vvf`) e poi applicare il Deployment.
    




finalmente... la configmap (cioè nel mio caso quella che si chiama appsettings-backend-ias) forse andava bene perchè è andato e in console leggo: 
```
PS C:\Progetti\BACKEND_IAS> kubectl get pods
NAME                                    READY   STATUS     RESTARTS      AGE
afm-mn-partnumbers-64b4477986-v4pw2     0/1     Init:0/1   0             17h
afm-mn-partnumbers-6697cfdbd7-7ntcs     1/1     Running    0             17h
afm-mn-serialnumbers-7f57d6c697-859wq   1/1     Running    0             45h
afm-ms-backend-55b89ddb56-tvfcn         1/1     Running    2 (18s ago)   62s
artemis-6b5f9f5989-6w6qq                1/1     Running    0             2d17h
bff-gateway-7c755dc647-ccvt8            1/1     Running    0             47h
debug-shell                             1/1     Running    0             46h
nginx-tcp-57f758fdb4-fwrkk              1/1     Running    0             2d17h
profile-manager-858b774f8b-c76xc        1/1     Running    0             21h
redis-master-0                          1/1     Running    0             2d17h
redis-replicas-0                        1/1     Running    0             2d17h
redis-replicas-1                        1/1     Running    0             2d17h
redis-sentinel-6b4f8c59db-qfnq4         1/1     Running    0             2d17h
redis-sentinel-6b4f8c59db-qlncm         1/1     Running    0             2d17h
redis-sentinel-6b4f8c59db-rgjld         1/1     Running    0             2d17h
redisinsight-d46789bdf-wwh9b            1/1     Running    0             2d17h
vvf-frontend-7fdf4d7656-znj55           0/1     Init:0/1   0             21m
PS C:\Progetti\BACKEND_IAS> kubectl describe pod afm-ms-backend-55b89ddb56-tvfcn
Name:             afm-ms-backend-55b89ddb56-tvfcn
Namespace:        vvf
Priority:         0
Service Account:  default
Node:             aks-d8sv5system-91347392-vmss000000/172.22.0.77
Start Time:       Thu, 18 Dec 2025 10:12:13 +0100
Labels:           app=afm-ms-backend
                  pod-template-hash=55b89ddb56
Annotations:      <none>
Status:           Running
IP:               10.240.1.132
IPs:
  IP:           10.240.1.132
Controlled By:  ReplicaSet/afm-ms-backend-55b89ddb56
Init Containers:
  init-copy-config:
    Container ID:  containerd://d772ffdd6bbc8744d099708834fd3882b21c3358fbfe68cbaf00929e8d312026
    Image:         busybox
    Image ID:      docker.io/library/busybox@sha256:d80cd694d3e9467884fcb94b8ca1e20437d8a501096cdf367a5a1918a34fc2fd
    Port:          <none>
    Host Port:     <none>
    Command:
      sh
      -c
      mkdir -p /app/config && cp /tmp/appsettings.json /app/config/
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Thu, 18 Dec 2025 10:12:35 +0100
      Finished:     Thu, 18 Dec 2025 10:12:35 +0100
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /mnt/config from afm-ms-backend-storage (rw)
      /tmp from config-source (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5wrwm (ro)
Containers:
  afm-ms-backend:
    Container ID:   containerd://0eca0b567c27ab7a1ae758d85daf2cf148f13fa7ffe74403e2728c3293e0f9e7
    Image:          afmvvf.azurecr.io/afm/afmmsbackend:latest
    Image ID:       afmvvf.azurecr.io/afm/afmmsbackend@sha256:e57d809a011776d4e814a53f2ee366e99e839832c6e421387e366d23bc04a807
    Port:           5021/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       CrashLoopBackOff
    Last State:     Terminated
      Reason:       Error
      Exit Code:    139
      Started:      Thu, 18 Dec 2025 10:13:10 +0100
      Finished:     Thu, 18 Dec 2025 10:13:26 +0100
    Ready:          False
    Restart Count:  2
    Environment:    <none>
    Mounts:
      /mnt/config from afm-ms-backend-storage (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5wrwm (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True
  Initialized                 True
  Ready                       False
  ContainersReady             False
  PodScheduled                True
Volumes:
  afm-ms-backend-storage:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  afm-ms-backend-pvc
    ReadOnly:   false
  config-source:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      appsettings-backend-ias
    Optional:  false
  kube-api-access-5wrwm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason                  Age                From                     Message
  ----     ------                  ----               ----                     -------
  Normal   Scheduled               95s                default-scheduler        Successfully assigned vvf/afm-ms-backend-55b89ddb56-tvfcn to aks-d8sv5system-91347392-vmss000000
  Normal   SuccessfulAttachVolume  75s                attachdetach-controller  AttachVolume.Attach succeeded for volume "pvc-07325c75-1b0d-41ba-83f5-8bd7b273004e"
  Normal   Pulling                 74s                kubelet                  Pulling image "busybox"
  Normal   Pulled                  73s                kubelet                  Successfully pulled image "busybox" in 819ms (819ms including waiting). Image size: 2224358 bytes.
  Normal   Created                 73s                kubelet                  Created container: init-copy-config
  Normal   Started                 73s                kubelet                  Started container init-copy-config
  Normal   Pulled                  72s                kubelet                  Successfully pulled image "afmvvf.azurecr.io/afm/afmmsbackend:latest" in 109ms (109ms including waiting). Image size: 113431546 bytes.
  Normal   Pulled                  61s                kubelet                  Successfully pulled image "afmvvf.azurecr.io/afm/afmmsbackend:latest" in 99ms (99ms including waiting). Image size: 113431546 bytes.
  Normal   Pulling                 38s (x3 over 72s)  kubelet                  Pulling image "afmvvf.azurecr.io/afm/afmmsbackend:latest"
  Normal   Started                 38s (x3 over 72s)  kubelet                  Started container afm-ms-backend
  Normal   Created                 38s (x3 over 72s)  kubelet                  Created container: afm-ms-backend
  Normal   Pulled                  38s                kubelet                  Successfully pulled image "afmvvf.azurecr.io/afm/afmmsbackend:latest" in 150ms (150ms including waiting). Image size: 113431546 bytes.
  Warning  BackOff                 8s (x3 over 50s)   kubelet                  Back-off restarting failed container afm-ms-backend in pod afm-ms-backend-55b89ddb56-tvfcn_vvf(61a3fdbc-b417-448e-8df3-19364bdaf3b0)
PS C:\Progetti\BACKEND_IAS> kubectl get pvc
NAME                       STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
afm-mn-partnumbers-pvc     Bound    pvc-015198b1-d403-4e8b-8eb5-1bcd1c0c6f3c   1Gi        RWO            default        <unset>                 47h
afm-mn-serialnumbers-pvc   Bound    pvc-efb2fdac-87e8-41fa-9995-a59ab085a2fd   1Gi        RWO            default        <unset>                 45h
afm-ms-backend-pvc         Bound    pvc-07325c75-1b0d-41ba-83f5-8bd7b273004e   1Gi        RWO            default        <unset>                 10m
afmvvf-frontend-pvc        Bound    pvc-dfae856e-ccac-420f-9ff2-7d0bd747ae66   1Gi        RWO            default        <unset>                 2d1h
artemis-data               Bound    pvc-db0b7c70-3e3e-4f93-92e1-bafc5b57fc28   5Gi        RWO            default        <unset>                 2d17h
data-redis-master-0        Bound    pvc-78c97698-7552-41a6-8db8-bd11def62c75   1Gi        RWO            default        <unset>                 2d17h
data-redis-replicas-0      Bound    pvc-7f64bba6-492e-4d9e-a76c-f3011fd6af88   8Gi        RWO            default        <unset>                 2d17h
data-redis-replicas-1      Bound    pvc-cae48f9c-f29f-41c4-bcb8-1cc58af29637   8Gi        RWO            default        <unset>                 2d17h
PS C:\Progetti\BACKEND_IAS>
```
adesso faccio gli altri seguendo i seguenti passaggi. 
elimino i volumi pv dal namespace default. 
fermo i pod (anzi cancello i deployment da dafault)
applico i volumi a vvf
applico il deploymetnt.yaml a vvf.

unico dubbio che ho è quello della conifgmap (= appsettings... quella che contiene le indicazioni per l'appsettings.json) , e il suo contenuto.. ieri ne ho applicate un paio alla configmap con il comando 
```
kubectl create configmap <nome-config> --from-file=appsettings.json
```
quindi non so se devo rifarlo. anche qui è rilevante il namespace? 



### aspettiamo.......


pipeline di Warehouse fatta


### call delle 11


![[Pasted image 20251218114122.png]]



### Test
Selenium.. leggi



## AKS
- partnumbers -> fatto 16 min fa
- backend -> errore redis
- frontend ci stanno lavorando -> non toccare 
- attachments **da fare** ma ci stanno lavorando -> non toccare
- serialnumbers **da fare** -> 
- installedparts 


TODO
fare pipeline di technical parts
modificare i deploy dei vari servizi (serialnumber partnumber ecc) e fare tutti gli altri deploy

